from pymilvus import connections,Collection,utility
from langchain_core.documents import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from vertexai.generative_models import GenerativeModel  #issue
from dotenv import load_dotenv
from src.step_5_prompt import prompt
from src.step_6_reranker import rerank_with_google

from src.llm_config import safety_settings, GENERATION_CONFIG, GENERATION_CONFIG1

from milvus_database.milvus_db import vector_search
from milvus_database.config import DB


import os
import gc
import time
import json

# from pydantic import BaseModel, Field
# from typing import Literal

# class LLMResponse(BaseModel):
#     Explanation: str = Field(..., description="Detailed explanation generated by the LLM.")
#     Summary: str = Field(..., description="Concise summary of the explanation.")
#     Follow_up: str = Field(..., description="Suggested follow-up question or action.")
#     table_data: str = Field(..., description="Tabular data in string format.")
#     Confidence_Reasoning: str = Field(..., description="Critical analysis of how much of the answer is actually found in the provided snippets.")
#     Confidence_Score: float = Field(..., ge=0.0, le=1.0, description="A decimal score between 0.0 and 1.0 indicating the confidence level of the response based on the provided context.")

load_dotenv()
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), "service-account.json")

def process_file(query, embedding_model,chat_history):
    """
    Match senior's function signature but adapted for JSON-based legal data.
    """
    query_vector = embedding_model.embed_query(query)
    try:
        
        print(f"\n Processing Query: {query}")

        results = vector_search(
            collection_name=DB.milvus_collection_name,
            partition_name=DB.default_partition,
            query_vectors=query_vector,
            num_results=30
        )

        hits = results[0] if results else []

        # Step 3: Build docs
        docs,meta_data = [],[]
        for hit in hits:
            entity = hit["entity"]
            text = entity.get("text", "")
            doc = Document(
                page_content=text,
                metadata={
                    "chapter": entity.get("chapter"),
                    "chapter_title": entity.get("chapter_title"),
                    "section": entity.get("section"),
                    "section_title": entity.get("section_title"),
                    "score": hit.get("distance")
                }
            )
            docs.append(doc)
            meta_data.append(doc.metadata)

        # Step 4: Rerank
        start_time = time.time()
        project_id = "km-judisasory"
        docs = rerank_with_google(query, docs, project_id)[:10]
        print("re ranking time", time.time()-start_time)

        # Step 5: Build context
        context_chunks = []
        for doc in docs:
            md = doc.metadata
            meta_line = f"[Meta: chapter={md['chapter']}, chapter_title={md['chapter_title']}, section={md['section']}, section_title={md['section_title']}]"
            context_chunks.append(f"{doc.page_content}-->{meta_line}\n\n")

        context = "\n\n".join(context_chunks)

        # Step 6: Generate LLM response
        formated_prompt = prompt.format(context=context, question=query,chat_history=chat_history)

        model = GenerativeModel("gemini-2.5-flash")
        result_new = model.generate_content(
            formated_prompt,
            generation_config=GENERATION_CONFIG,
            safety_settings=safety_settings,
        )

        # print("result_new:", result_new)

        response = result_new.candidates[0].content.parts[0].text
        # logprobs_sequence = result_new.candidates[0].logprobs_result.chosen_candidates
        print("type of response:", type(response))
        print("\n Response:", response)
        # print("\n type of Logprobs:", type(logprobs_sequence))

        

        # def find_confidence_score(context, response_text):
        #     # Use double curly braces {{ }} for the JSON structure inside f-string
        #     audit_prompt = f"""
        #     ### CONTEXT:
        #     {context}

        #     ### ANSWER:
        #     {response_text}

        #     ### ROLE
        #     You are a High-Precision Legal Auditor. Your goal is to evaluate the relationship between a retrieved Legal Context and the Answer you generate. You must provide a "Confidence_Score" as a decimal between 0.00 and 1.00.

        #     ### TASK
        #     1. Analyze the retrieved Context chunks.
        #     2. Generate a legally accurate response.
        #     3. Perform a "Faithfulness Audit" to calculate the Confidence_Score based on the SCORING RULES below.

        #     ### FAITHFULNESS AUDIT LOGIC
        #     To calculate the decimal, you must follow this internal math:
        #     - START at 1.00.
        #     - SUBTRACT 0.12 if the answer relies on synthesizing multiple chunks rather than one clear statement.
        #     - SUBTRACT 0.15 if a specific Section Number (e.g., BNS Sec 42) is missing from the Context but mentioned in your answer.
        #     - SUBTRACT 0.20 if you had to use external legal knowledge to make the answer coherent.
        #     - SUBTRACT 0.05 for every "qualifier" used (e.g., "likely," "probably," "appears to be").
        #     - SUBTRACT 0.10 if the Context is truncated or contains OCR noise.

        #     ### SCORING RULES
        #     - 0.90 - 1.00: Verbatim or near-verbatim answers found in a single, clear chunk.
        #     - 0.70 - 0.85: Well-supported answers requiring synthesis of 2-3 chunks. (Default for good RAG).
        #     - 0.40 - 0.69: Related context that doesn't explicitly answer the core query.
        #     - 0.00 - 0.39: Irrelevant or contradictory context.

        #     CRITICAL: Avoid rounding to 0.5, 0.9, or 1.0. Aim for precise decimals like 0.73, 0.84, or 0.62 based on your math.

        #     ### OUTPUT FORMAT (JSON ONLY)
            
        #     "Confidence_Reasoning": "Briefly list the deductions made...",
        #     "Confidence_Score": 0.83
            
        #     """

        #     try:
        #         # Use the same model instance defined globally or re-init
        #         llm_response = model.generate_content(
        #             audit_prompt,
        #             generation_config=GENERATION_CONFIG1,
        #             safety_settings=safety_settings,
        #         )
        #         return llm_response.text
        #     except Exception as e:
        #         print(f"Audit API Error: {e}")
        #         return None

        # # Call with BOTH context and response
        # ans_raw = find_confidence_score(context, response)

        # if ans_raw:
        #     try:
        #         # Clean the string in case the model added markdown backticks
        #         clean_json = ans_raw.replace("```json", "").replace("```", "").strip()
        #         audit_results = json.loads(clean_json)
        #         conf_score = audit_results.get("Confidence_Score", 0.0)
        #         conf_reasoning = audit_results.get("Confidence_Reasoning", "Audit failed to parse.")
        #     except Exception as e:
        #         print(f"Failed to parse JSON: {e}")
        #         conf_score, conf_reasoning = 0.0, "Parsing Error"
        # else:
        #     conf_score, conf_reasoning = 0.0, "API Error"

        # print(f"Confidence: {conf_score} | Reasoning: {conf_reasoning}")
        
        final_output = {
            "query": query,
            "response": response,       
            "metadata": meta_data,
            # "confidence_score": new_response.Confidence_Score
        }
        # print("ðŸ’¾ Saving new result to Semantic Cache...")
        # upsert_rag_response(
        #     rag_answer=final_output,
        #     query_text = query,
        #     query_vector=query_vector,
        #     id_generator=id_gen
        # )
        return final_output

    except Exception as e:
        print(f" Error processing query: {e}")
        return None

# # --- Example Usage ---
# if __name__ == "__main__":
#     sample_query = "What does Section 1 of Bharatiya Nyaya Sanhita state?"
#     output = process_file(sample_query)
#     if output:
#         print("\n--- Response ---")
#         print(output["response"])
